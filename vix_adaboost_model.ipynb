{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bb18e4c",
   "metadata": {},
   "source": [
    "# adaBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc130475",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "142932a5",
   "metadata": {},
   "source": [
    "#### to do:\n",
    "\n",
    "Add VIX and VIX return in the correlation_filter \n",
    "\n",
    "manage names in the features: spy_volume, spy_return, spy_close, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "57857e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import appropriate modules\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler,OneHotEncoder, MinMaxScaler\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "import datetime\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "from datetime import datetime\n",
    "from pandas.tseries.offsets import DateOffset\n",
    "import hvplot\n",
    "import hvplot.pandas\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from vix_functions import garch_fit_and_predict, correlation_filter, retrieve_yahoo_close, retrieve_yahoo_volume \n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "221afe71",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold= 0 # 0.030\n",
    "adaboost_estimators = 22\n",
    "training_period_months = 120\n",
    "# Inclusion of the first 4 components lag1\n",
    "#n: number of components to include\n",
    "number_of_pca_lag_component_to_include = 4\n",
    "num_pca_components = 40\n",
    "demo_mode = True\n",
    "parameter_tuning_mode = False\n",
    "run_multiple_tuning_iterations = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8645f73b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "386232fe",
   "metadata": {},
   "source": [
    "# Generation of the Features Matrix X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "9426641b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ticker List: VIX must be in first position\n",
    "ticker_list= [\"^VIX\", \"spy\", 'XLF', 'XLE',\n",
    "              'EURUSD=X', 'GBPUSD=X', 'AUDUSD=X', 'BRLUSD=X', \"DX-Y.NYB\",\"USDJPY=X\", \n",
    "              '^TNX', 'ZB=F', 'ZF=F', 'NQ=F','NKD=F',\n",
    "              'LQD',\n",
    "              'AAPL', 'AMZN', 'GE','MU','MSFT', 'BMY', 'FDX', 'GS','PLD','NVDA',   \n",
    "              \"tlt\", \"ief\", \n",
    "              \"FXI\", \"EZU\", \"EEM\", \"EFA\", 'FEZ', \"^GDAXI\", '^FTSE','^HSI','^FCHI',\n",
    "              \"gld\", \"slv\", \"CL=F\"]\n",
    "    \n",
    "#\n",
    "# Energy Select Sector SPDR Fund (XLE)\n",
    "# Financial Select Sector SPDR Fund (XLF)\n",
    "# CAC 40 (^FCHI)\n",
    "# Yen Denominated TOPIX Futures,D (TPY=F)\n",
    "# HANG SENG INDEX (^HSI)\n",
    "# FTSE 100 (^FTSE)\n",
    "# DAX PERFORMANCE-INDEX (^GDAXI)\n",
    "# SPDR EURO STOXX 50 ETF (FEZ)\n",
    "# Nikkei/USD Futures,Dec-2021 (NKD=F)\n",
    "# Nasdaq 100 Dec 21 (NQ=F)\n",
    "# NVIDIA Corporation (NVDA)\n",
    "# 'WTI CRUDE FUTURE' '261220.KS' -- eliminate\n",
    "# Euro spot  'EURUSD'\n",
    "# Treasury Yield 10 Years (^TNX) -- 1985\n",
    "# U.S. Treasury Bond Futures,Dec- (ZB=F) - 2000\n",
    "# Five-Year US Treasury Note Futu (ZF=F) - 2000\n",
    "# American Funds U.S. Government Securities Fund Class C (UGSCX) - 2001\n",
    "# PIMCO High Yield Municipal Bond Fund Class A (PYMAX) --eliminate\n",
    "# Vanguard High-Yield Corporate Fund Investor Shares (VWEHX) -- 1985 --eliminate\n",
    "# iShares iBoxx $ Investment Grade Corporate Bond ETF (LQD)\n",
    "# 13 Week Treasury Bill (^IRX) --eliminate\n",
    "# Micron Technology, Inc. (MU)\n",
    "# Microsoft Corporation (MSFT)\n",
    "# Bristol-Myers Squibb Company (BMY)\n",
    "# FEDEX CORP (FDX)\n",
    "# The Goldman Sachs Group, Inc. (GS)\n",
    "# Prologis, Inc. (PLD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "cf7de2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "4913c392",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "4b8c26e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed retrieve of close prices\n"
     ]
    }
   ],
   "source": [
    "# Inclusion of security levels X1\n",
    "def retrieve_close(close_prices_dict, ticker_list):\n",
    "    for ticker in ticker_list:\n",
    "        close_price = retrieve_yahoo_close(ticker, start_date='2006-07-02', end_date='2021-10-02')\n",
    "        close_prices_dict[ticker] = close_price\n",
    "    return close_prices_dict\n",
    "\n",
    "\n",
    "if demo_mode == True:\n",
    "    close_prices_df = pd.read_csv(\"adaboost_close_prices.csv\", index_col=\"Date\", parse_dates=True, infer_datetime_format=True)\n",
    "else:\n",
    "    close_prices_dict = {}\n",
    "    close_prices_dict = retrieve_close(close_prices_dict, ticker_list)\n",
    "    close_prices_df= pd.DataFrame(close_prices_dict)\n",
    "    close_prices_df.to_csv(\"adaboost_close_prices.csv\", index=True)\n",
    "print(\"Completed retrieve of close prices\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "bc1ab752",
   "metadata": {},
   "outputs": [],
   "source": [
    "close_prices_df=close_prices_df.ffill(axis='rows')\n",
    "close_prices_df=close_prices_df.bfill(axis='rows')\n",
    "\n",
    "close_prices_component_df = correlation_filter(close_prices_df, min_corr=0.20, key_column='^VIX', eliminate_first_column=True)\n",
    "\n",
    "X1=close_prices_component_df\n",
    "vix=close_prices_df['^VIX']\n",
    "vix_ret=close_prices_df['^VIX'].pct_change()\n",
    "VIX=pd.DataFrame([vix, vix_ret]).T\n",
    "VIX.columns=['VIX','VIX_ret']\n",
    "\n",
    "X1=pd.concat([VIX,close_prices_component_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "6faeb527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inclusion of security returns X2\n",
    "# Include returns that are correlated more than 0.20 with the Vix return\n",
    "\n",
    "security_returns_df= close_prices_df.pct_change()\n",
    "security_returns_component_df = correlation_filter(\n",
    "                                        security_returns_df, \n",
    "                                        min_corr=0.20, \n",
    "                                        key_column='^VIX', \n",
    "                                        eliminate_first_column=True \n",
    ")\n",
    "\n",
    "X2=security_returns_component_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecc530f",
   "metadata": {},
   "source": [
    "#### OBSERVATION: It would be good to include VIX level and Return and check on both in case correlations changes in the future. As of now with some variables I check with VIX level, and others I check VIX return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "8d420c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed retrieve of volume\n"
     ]
    }
   ],
   "source": [
    "# inclusion of security volume X3\n",
    "volume_list = ticker_list[1:len(ticker_list)]\n",
    "\n",
    "def retrieve_volume(volume_dict, volume_list):\n",
    "    for ticker in volume_list:        \n",
    "        volume = retrieve_yahoo_volume(ticker)\n",
    "        volume_dict[ticker] = volume\n",
    "    return volume_dict\n",
    "\n",
    "if demo_mode == True:\n",
    "    volume_df = pd.read_csv(\"adaboost_volume.csv\", index_col=\"Date\", parse_dates=True, infer_datetime_format=True)\n",
    "else:\n",
    "    volume_dict = {}\n",
    "    volume_dict = retrieve_volume(volume_dict, volume_list)\n",
    "    volume_df= pd.DataFrame(volume_dict)\n",
    "    volume_df.to_csv(\"adaboost_volume.csv\", index=True)\n",
    "print(\"Completed retrieve of volume\")\n",
    "\n",
    "volume_df_with_vix=pd.concat([vix, volume_df], axis=1)\n",
    "#print(volume_df_with_vix.corr())\n",
    "\n",
    "volume_component_df = correlation_filter(volume_df_with_vix, min_corr=0.20, key_column='^VIX', eliminate_first_column=True )\n",
    "X3=volume_component_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "87b570bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting series: spy\n",
      "starting series: XLF\n",
      "starting series: XLE\n",
      "starting series: EURUSD=X\n",
      "starting series: GBPUSD=X\n",
      "starting series: AUDUSD=X\n",
      "starting series: BRLUSD=X\n",
      "starting series: DX-Y.NYB\n",
      "starting series: USDJPY=X\n",
      "starting series: ^TNX\n",
      "starting series: ZB=F\n",
      "starting series: ZF=F\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sangrams\\Anaconda_with_python_3.8\\envs\\dev\\lib\\site-packages\\arch\\univariate\\base.py:310: DataScaleWarning: y is poorly scaled, which may affect convergence of the optimizer when\n",
      "estimating the model parameters. The scale of y is 0.05177. Parameter\n",
      "estimation work better when this value is between 1 and 1000. The recommended\n",
      "rescaling is 10 * y.\n",
      "\n",
      "This warning can be disabled by either rescaling y before initializing the\n",
      "model or by setting rescale=False.\n",
      "\n",
      "  data_scale_warning.format(orig_scale, rescale), DataScaleWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting series: NQ=F\n",
      "starting series: NKD=F\n",
      "starting series: LQD\n",
      "starting series: AAPL\n",
      "starting series: AMZN\n",
      "starting series: GE\n",
      "starting series: MU\n",
      "starting series: MSFT\n",
      "starting series: BMY\n",
      "starting series: FDX\n",
      "starting series: GS\n",
      "starting series: PLD\n",
      "starting series: NVDA\n",
      "starting series: tlt\n",
      "starting series: ief\n",
      "starting series: FXI\n",
      "starting series: EZU\n",
      "starting series: EEM\n",
      "starting series: EFA\n",
      "starting series: FEZ\n",
      "starting series: ^GDAXI\n",
      "starting series: ^FTSE\n",
      "starting series: ^HSI\n",
      "starting series: ^FCHI\n",
      "starting series: gld\n",
      "starting series: slv\n",
      "starting series: CL=F\n"
     ]
    }
   ],
   "source": [
    "# Inclusion of GARCH series X4\n",
    "garch_series=pd.DataFrame()\n",
    "\n",
    "not_to_include=['^VIX']\n",
    "\n",
    "\n",
    "for ticker in ticker_list:\n",
    "    \n",
    "        if ticker in not_to_include:\n",
    "            continue\n",
    "    \n",
    "        if (np.mean(abs(security_returns_df[ticker])))<=0.1:\n",
    "            security_returns_df[ticker]=10*security_returns_df[ticker]\n",
    "        if demo_mode == True:\n",
    "            print_series = False\n",
    "        else:\n",
    "            print_series = True\n",
    "        garch_series[ticker]=garch_fit_and_predict(security_returns_df[ticker], ticker, horizon=1, p=1, q=1, o=1, print_series_name=print_series)\n",
    "\n",
    "        if (np.mean(abs(security_returns_df[ticker])))<=0.1:\n",
    "            garch_series[ticker]=100*garch_series[ticker]\n",
    "            \n",
    "        \n",
    "X4=garch_series\n",
    "if demo_mode == False:\n",
    "    X4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "8156913c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inclusion of return squares in X5\n",
    "\n",
    "returns_squared_df_no_vix= security_returns_df.drop(columns='^VIX')**2\n",
    "returns_squared_and_vix_level_df=pd.concat([vix,returns_squared_df_no_vix], axis=1)\n",
    "returns_squared_component_df = correlation_filter(returns_squared_and_vix_level_df, min_corr=0.20, key_column='^VIX', eliminate_first_column=True)\n",
    "\n",
    "X5=returns_squared_component_df\n",
    "if demo_mode == False:\n",
    "    X5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174f57d9",
   "metadata": {},
   "source": [
    "### Inclusion of Google Trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "21838bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload of Google Tremds -- X6\n",
    "keywords=['banking', \"consumer\", \"depression\", \"gdp\", \"inflation\",\n",
    "          'unemployment', 'liquidity','cci', 'vix_word'] #'jobless_claims_pao', \n",
    "google_trends_df=pd.DataFrame()\n",
    "\n",
    "for keyword in keywords:\n",
    "    file_path=f\"./Resources/Google_trends/{keyword}.csv\"\n",
    "    if demo_mode == False:\n",
    "        print(file_path)\n",
    "    trend=pd.read_csv(Path(file_path),\n",
    "                      index_col= 'Daily', \n",
    "                      parse_dates= True,\n",
    "                      infer_datetime_format=True\n",
    "                     )\n",
    "    #print(trend)\n",
    "    google_trends_df=pd.concat([google_trends_df, trend], axis=1)\n",
    "    #print(google_trends_df)\n",
    "\n",
    "if demo_mode == False:\n",
    "    google_trends_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "19f06ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working on preparing Google-trends data\n",
    "\n",
    "# Unifying google dates with VIX\n",
    "minimum_date=vix.index.min()\n",
    "maximum_date=vix.index.max()\n",
    "\n",
    "google_trends_df=google_trends_df.loc[minimum_date:maximum_date,:]\n",
    "#print(google_trends_df.iloc[0,:])\n",
    "\n",
    "vix_google_trends_df=pd.concat([vix, google_trends_df], axis=1)\n",
    "vix_google_trends_df.isna().sum()\n",
    "\n",
    "#print(vix_google_trends_df.head())\n",
    "\n",
    "#vix_google_trends_df=vix_google_trends_df.fillna(0)\n",
    "#vix_google_trends_df\n",
    "#vix_google_trends_df.loc[vix_google_trends_df['^VIX'].isna(),['^VIX','Banking: (United States)']]\n",
    "\n",
    "# We will drop Saturday Sunday, but we would like to average Fri-Sat-Sun and reset the value of Friday\n",
    "vix_google_trends_df=vix_google_trends_df.dropna()\n",
    "google_trends_df=vix_google_trends_df.iloc[:,1:]\n",
    "#google_trends_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "8dace168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering by correlation X6\n",
    "\n",
    "google_trends_component_df = correlation_filter(\n",
    "                                vix_google_trends_df, \n",
    "                                min_corr=0.05, \n",
    "                                key_column='^VIX', \n",
    "                                eliminate_first_column=True)\n",
    "\n",
    "X6=google_trends_component_df\n",
    "\n",
    "# We will interpolate so we can fill the missing data only on Google Trends\n",
    "pro_interpolation_of_X6=pd.concat([vix, X6], axis=1)\n",
    "pro_interpolation_of_X6=pro_interpolation_of_X6.interpolate(method=\"polynomial\", order=2, axis=0)\n",
    "pro_interpolation_of_X6\n",
    "X6 = pro_interpolation_of_X6.iloc[:,1:]\n",
    "if demo_mode == False:\n",
    "    X6.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c95cdd",
   "metadata": {},
   "source": [
    "# INCLUSION OF LAGGED SERIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "80631013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inclusion of different lags of data from the X -- up to n_lag\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c237c7",
   "metadata": {},
   "source": [
    "# Inclusion of Economic and Financial Series X7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "2d299913",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Economic Series\n",
    "# Upload of csv files -- X7\n",
    "keywords=['JobClaimsWeeklySeries', 'vix_put_call_ratio']\n",
    "economic_series_df=pd.DataFrame()\n",
    "\n",
    "for keyword in keywords:\n",
    "    file_path=f\"./Resources/Economic_and_financial_Series/{keyword}.csv\"\n",
    "    if demo_mode == False:\n",
    "        print(file_path)\n",
    "    new_series=pd.read_csv(Path(file_path),\n",
    "                      index_col= 'DATE', \n",
    "                      parse_dates= True,\n",
    "                      infer_datetime_format=True\n",
    "                     )\n",
    "    if keyword=='JobClaimsWeeklySeries':\n",
    "        new_series=new_series.shift(-1, freq='D')\n",
    "    if demo_mode == False:\n",
    "        print(new_series)\n",
    "    # Adjustment due to weekend data. We are going to assign data on the weekends to Friday, since are going to be \n",
    "    # consider for the the prediction of Monday\n",
    "    economic_series_df=pd.concat([economic_series_df, new_series], axis=1)\n",
    "    #print(economic_series_df.tail())\n",
    "\n",
    "economic_series_df\n",
    "economic_series_change_df = economic_series_df.pct_change().add_suffix('_change')\n",
    "\n",
    "if demo_mode == False:\n",
    "    economic_series_df.loc[:,:].tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "13f1c2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working on indexes available\n",
    "\n",
    "# Changes of columns that are on a weekend - concat with vix to add week days\n",
    "vix_economic= pd.concat([vix,economic_series_df,economic_series_change_df ],axis=1)\n",
    "vix_economic['VIX Put/Call Ratio']= vix_economic['VIX Put/Call Ratio'].fillna(0)\n",
    "\n",
    "# Applying interpolation to appropiate columns. Levels: interpolation, changes: zeros\n",
    "vix_economic.loc[:,economic_series_df.columns]=vix_economic.loc[:,economic_series_df.columns].interpolate(method=\"polynomial\", order=2, axis=0)\n",
    "vix_economic.loc[:,economic_series_change_df.columns]=vix_economic.loc[:,economic_series_change_df.columns].fillna(0)\n",
    "\n",
    "#print(vix_economic)\n",
    "\n",
    "#Filtering for available dates\n",
    "economic_series_ready_df = vix_economic.loc[minimum_date:maximum_date,:]\n",
    "economic_series_ready_df = economic_series_ready_df.iloc[:,1:]\n",
    "\n",
    "X7=economic_series_ready_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "a4ba8a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPY volatility on varios tracks X8\n",
    "\n",
    "if demo_mode == True:\n",
    "    close_price_spy_df = pd.read_csv(\"adaboost_spy_data.csv\", index_col=\"Date\", parse_dates=True, infer_datetime_format=True)\n",
    "else:\n",
    "    close_price_spy_df = retrieve_yahoo_close('spy', start_date='2005-01-01', end_date='2021-10-02')\n",
    "    close_price_spy_df.to_csv(\"adaboost_spy_data.csv\", index=True)\n",
    "spy_returns_df=close_price_spy_df.pct_change()\n",
    "\n",
    "spy_volatility_30_days = spy_returns_df.rolling(window=30).std()\n",
    "spy_volatility_60_days = spy_returns_df.rolling(window=60).std()\n",
    "spy_volatility_90_days = spy_returns_df.rolling(window=90).std()\n",
    "spy_volatility_260_days = spy_returns_df.rolling(window=260).std()\n",
    "spy_volatility_360_days = spy_returns_df.rolling(window=360).std()\n",
    "spy_volatility_10_days = spy_returns_df.rolling(window=10).std()\n",
    "spy_volatility_20_days = spy_returns_df.rolling(window=20).std()\n",
    "spy_volatility_180_days = spy_returns_df.rolling(window=180).std()\n",
    "spy_volatility_200_days = spy_returns_df.rolling(window=200).std()\n",
    "spy_volatility_120_days = spy_returns_df.rolling(window=120).std()\n",
    "\n",
    "if demo_mode == False:\n",
    "    display(spy_volatility_120_days.shape)\n",
    "    display(security_returns_df['spy'].shape)\n",
    "\n",
    "X8=pd.concat([security_returns_df['spy'],\n",
    "              spy_volatility_30_days ,\n",
    "              spy_volatility_60_days,\n",
    "              spy_volatility_90_days,\n",
    "              spy_volatility_260_days,\n",
    "              spy_volatility_360_days,\n",
    "              spy_volatility_10_days,\n",
    "               spy_volatility_20_days,\n",
    "               spy_volatility_180_days,\n",
    "               spy_volatility_200_days,\n",
    "              spy_volatility_120_days],\n",
    "             axis=1\n",
    "                )\n",
    "X8=X8.loc[minimum_date:maximum_date,:]\n",
    "if demo_mode == False:\n",
    "    display(X8.shape)\n",
    "X8=X8.ffill()\n",
    "X8=X8.iloc[:,1:]\n",
    "if demo_mode == False:\n",
    "    X8.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "29c96cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inclusion of Technical Indicators\n",
    "technical_indicators = pd.read_csv(\"technical_indicators.csv\", index_col=\"Date\", parse_dates=True, infer_datetime_format=True)\n",
    "technical_indicators = technical_indicators.drop(columns=[\"vix close\", \"vix return\", \"mean\"])\n",
    "X9 = pd.concat([security_returns_df['spy'], technical_indicators], axis=1)\n",
    "X9=X9.loc[minimum_date:maximum_date,:]\n",
    "if demo_mode == False:\n",
    "    display(X9.shape)\n",
    "X9=X9.ffill()\n",
    "X9=X9.iloc[:,1:]\n",
    "if demo_mode == False:\n",
    "    display(X9.shape)\n",
    "# display(X9.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "812907dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inclusion of day of week data\n",
    "day_of_week = pd.read_csv(\"prophet_output_day_of_week.csv\", index_col=\"Date\", parse_dates=True, infer_datetime_format=True)\n",
    "day_of_week = day_of_week.drop(columns=[\"y\"])\n",
    "X10 = pd.concat([security_returns_df['spy'], day_of_week], axis=1)\n",
    "X10 = X10.loc[minimum_date:maximum_date,:]\n",
    "if demo_mode == False:\n",
    "    display(X10.shape)\n",
    "X10 = X10.ffill()\n",
    "X10 = X10.iloc[:,1:]\n",
    "if demo_mode == False:\n",
    "    display(X10.shape)\n",
    "#display(X10.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "9b7a4acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inclusion of neural network data\n",
    "predictions_train_test_df = pd.read_csv(\"neural_network_output.csv\", index_col=\"Date\", parse_dates=True, infer_datetime_format=True)\n",
    "X11 = pd.concat([security_returns_df['spy'], predictions_train_test_df], axis=1)\n",
    "X11 = X11.loc[minimum_date:maximum_date,:]\n",
    "if demo_mode == False:\n",
    "    display(X11.shape)\n",
    "X11 = X11.ffill()\n",
    "X11 = X11.fillna(0)\n",
    "X11 = X11.iloc[:,1:]\n",
    "if demo_mode == False:\n",
    "    display(X11.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84913d3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bfaeb04e",
   "metadata": {},
   "source": [
    "# GENERATION OF THE FEATURE MATRIX **X**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "756d1b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XY.shape: (3980, 201), 2006-07-03 00:00:00, 2021-10-01 00:00:00 \n",
      "X1.shape: (3980, 21), 2006-07-03 00:00:00, 2021-10-01 00:00:00 \n",
      "X2.shape: (3980, 28), 2006-07-03 00:00:00, 2021-10-01 00:00:00 \n",
      "X3.shape: (3980, 21), 2006-07-03 00:00:00, 2021-10-01 00:00:00 \n",
      "X4.shape: (3979, 39), 2006-07-03 00:00:00, 2021-10-01 00:00:00 \n",
      "X5.shape: (3980, 33), 2006-07-03 00:00:00, 2021-10-01 00:00:00 \n",
      "X6.shape: (3980, 3), 2006-07-03 00:00:00, 2021-10-01 00:00:00 \n",
      "X7.shape: (3980, 4), 2006-07-03 00:00:00, 2021-10-01 00:00:00 \n",
      "X8.shape: (3980, 10), 2006-07-03 00:00:00, 2021-10-01 00:00:00 \n",
      "X9.shape: (3980, 36), 2006-07-03 00:00:00, 2021-10-01 00:00:00 \n",
      "X10.shape: (3980, 5), 2006-07-03 00:00:00, 2021-10-01 00:00:00 \n",
      "X11.shape: (3980, 1), 2006-07-03 00:00:00, 2021-10-01 00:00:00 \n"
     ]
    }
   ],
   "source": [
    "# Concatenation of all sources of data\n",
    "XY=pd.concat([X1, X2, X3, X4, X5, X6, X7, X8, X9, X10, X11], axis=1)\n",
    "if parameter_tuning_mode == True:\n",
    "    print(XY.shape)\n",
    "\n",
    "XY.dropna(subset = ['VIX', 'VIX_ret'])\n",
    "if parameter_tuning_mode == True:\n",
    "    print(XY.shape)\n",
    "\n",
    "# Interpolation is not applied to numerical variables. We are just going to drop those.\n",
    "print(f\"XY.shape: {XY.shape}, {XY.index.min()}, {XY.index.max()} \")\n",
    "print(f\"X1.shape: {X1.shape}, {XY.index.min()}, {XY.index.max()} \")\n",
    "print(f\"X2.shape: {X2.shape}, {XY.index.min()}, {XY.index.max()} \")\n",
    "print(f\"X3.shape: {X3.shape}, {XY.index.min()}, {XY.index.max()} \")\n",
    "print(f\"X4.shape: {X4.shape}, {XY.index.min()}, {XY.index.max()} \")\n",
    "print(f\"X5.shape: {X5.shape}, {XY.index.min()}, {XY.index.max()} \")\n",
    "print(f\"X6.shape: {X6.shape}, {XY.index.min()}, {XY.index.max()} \")\n",
    "print(f\"X7.shape: {X7.shape}, {XY.index.min()}, {XY.index.max()} \")\n",
    "print(f\"X8.shape: {X8.shape}, {XY.index.min()}, {XY.index.max()} \")\n",
    "print(f\"X9.shape: {X9.shape}, {XY.index.min()}, {XY.index.max()} \")\n",
    "print(f\"X10.shape: {X10.shape}, {XY.index.min()}, {XY.index.max()} \")\n",
    "print(f\"X11.shape: {X11.shape}, {XY.index.min()}, {XY.index.max()} \")\n",
    "\n",
    "#display(XY.isna().head(40))\n",
    "#display(XY.isna().sum().tail(40))\n",
    "#XY=XY.dropna()\n",
    "if parameter_tuning_mode == True:\n",
    "    XY.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7c5dc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "e75ddde4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2006-07-04    0.000000\n",
       "2006-07-05    0.084291\n",
       "2006-07-07    0.023443\n",
       "2006-07-10    0.003579\n",
       "2006-07-12    0.102740\n",
       "                ...   \n",
       "2021-09-17    0.113430\n",
       "2021-09-20    0.235464\n",
       "2021-09-27    0.056901\n",
       "2021-09-28    0.239339\n",
       "2021-09-30    0.025709\n",
       "Name: VIX_ret, Length: 1912, dtype: float64"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the Signal column\n",
    "vix_ret=vix.pct_change()\n",
    "XY[\"Signal\"] = 0.0\n",
    "\n",
    "XY.loc[(XY['VIX_ret'] >= threshold), 'Signal'] = 1\n",
    "\n",
    "# # Generate the trading signals 1 (entry) or -1 (exit)\n",
    "# # where 1 is when the ^VIX is greater than 3.6%.\n",
    "# # where 0 is when the ^VIX  is less than 3.6%.\n",
    "#for index, row in XY.iterrows():\n",
    "#    if row[\"VIX_ret\"] >= 0.036:\n",
    "#        XY.loc[index, \"Signal\"] = 1.0\n",
    "\n",
    "# Review the DataFrame\n",
    "if parameter_tuning_mode == True:\n",
    "    print(XY[\"Signal\"].head())\n",
    "    XY[\"Signal\"].value_counts()\n",
    "XY.loc[XY[\"Signal\"]==1, 'VIX_ret']\n",
    "#XY.shape  ## 3981"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "72522851",
   "metadata": {},
   "outputs": [],
   "source": [
    "vix_ret=vix.pct_change()\n",
    "vix_ret[vix_ret>=threshold].index\n",
    "vix_ret.shape\n",
    "\n",
    "\n",
    "# How many values of the vix we missed due to missing data on other series\n",
    "compare=pd.concat([XY.loc[XY[\"Signal\"]==1, 'VIX_ret'],vix_ret[vix_ret>=threshold] ], axis=1)\n",
    "missing_dates=compare.loc[compare[\"VIX_ret\"]!=compare[\"^VIX\"]]\n",
    "missing_dates=missing_dates.index\n",
    "missing_dates\n",
    "if parameter_tuning_mode == True:\n",
    "    vix[missing_dates]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "cc512aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the target set y using the Signal column\n",
    "y = XY[\"Signal\"]\n",
    "# Display a sample of y\n",
    "if parameter_tuning_mode == True:\n",
    "    y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "5982cc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if parameter_tuning_mode == True:\n",
    "    display(y.isna().sum())\n",
    "    display(y.shape)\n",
    "    display(XY.shape)\n",
    "    display(XY.drop(columns=[\"Signal\"]).isna().sum())\n",
    "    display(XY.drop(columns=['Signal']).shift().isna().sum())\n",
    "    display(XY.drop(columns=['Signal']).shift().dropna().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "65be35aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "XY_modified = XY.shift().dropna()\n",
    "if parameter_tuning_mode == True:\n",
    "    display(XY_modified.shape)\n",
    "\n",
    "y = XY_modified[\"Signal\"].shift(-1)\n",
    "\n",
    "X = XY_modified\n",
    "\n",
    "if parameter_tuning_mode == True:\n",
    "    display(y.shape)\n",
    "    display(X.shape)\n",
    "    display(y.isna().sum())\n",
    "    display(X.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "9350ab28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review the features DataFrame\n",
    "if parameter_tuning_mode == True:\n",
    "    X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "b5b52539",
   "metadata": {},
   "outputs": [],
   "source": [
    "if parameter_tuning_mode == True:\n",
    "    X.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "247261e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_training_test_data(X, y):\n",
    "    # Split the preprocessed data into a training and testing dataset\n",
    "    # Assign the function a random_state equal to 1\n",
    "    training_begin = X.index.min()\n",
    "    training_end = X.index.min() + DateOffset(months=training_period_months)\n",
    "\n",
    "    X_train = X.loc[training_begin:training_end]\n",
    "    y_train = y.loc[training_begin:training_end]\n",
    "\n",
    "    X_test = X.loc[training_end + DateOffset(days=1):]\n",
    "    y_test = y.loc[training_end + DateOffset(days=1):]\n",
    "\n",
    "    if parameter_tuning_mode == True:\n",
    "        print(f\"Training dates: {training_begin} to {training_end}\")\n",
    "        display(y_train.value_counts())\n",
    "        display(y_test.shape)\n",
    "        display(X_test.shape)\n",
    "        display(X_train.shape)\n",
    "        display(y_train.shape)\n",
    "        display(X_train.tail(1))\n",
    "        display(X_test.head(1))\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "X_train, y_train, X_test, y_test = split_training_test_data(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "0ac4507f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_scale(X_train, X_test):\n",
    "    # Create a StandardScaler instance\n",
    "    scaler =  StandardScaler() # MinMaxScaler() #\n",
    " \n",
    "    # Apply the scaler model to fit the X-train data\n",
    "    X_scaler = scaler.fit(X_train)\n",
    "\n",
    "    # Transform the X_train and X_test DataFrames using the X_scaler\n",
    "    X_train_scaled = X_scaler.transform(X_train)\n",
    "    X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "    if parameter_tuning_mode == True:\n",
    "        display(X_train_scaled.shape)\n",
    "        display(X_test_scaled.shape)\n",
    "    return X_train_scaled, X_test_scaled\n",
    "    \n",
    "X_train_scaled, X_test_scaled = standard_scale(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "1a33e31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaboost_pca(X_train, X_test):\n",
    "    pca = PCA(n_components = num_pca_components)\n",
    "    pca.fit(X_train)\n",
    "    \n",
    "    principal_components_train = pca.transform(X_train)\n",
    "    principal_components_test  = pca.transform(X_test)\n",
    "    \n",
    "    pca_column_list = []\n",
    "    for i in range(1, num_pca_components+1):\n",
    "        pca_column_list.append(f\"pca{i}\")\n",
    "\n",
    "    principal_components_train_test = np.concatenate((principal_components_train, principal_components_test), axis = 0)\n",
    "    principal_components_train_test_df = pd.DataFrame(data = principal_components_train_test, columns = pca_column_list, index = X.index)\n",
    "    if parameter_tuning_mode == True:\n",
    "        display(sum(pca.explained_variance_ratio_))\n",
    "        display(principal_components_train_test_df.shape)\n",
    "        display(principal_components_train_test_df.head(5))\n",
    "    return principal_components_train_test_df\n",
    "principal_components_train_test_df = adaboost_pca(X_train_scaled, X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "853d86ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shift the LAG Component by a certain amount\n",
    "def create_pca_lag(principal_components_train_test_df, shift_amount):\n",
    "    X_pc_lag = principal_components_train_test_df.iloc[:,0:(number_of_pca_lag_component_to_include-1)]\n",
    "    if parameter_tuning_mode == True:\n",
    "        display(X_pc_lag.shape)\n",
    "\n",
    "    X_pc_lag.columns = ['pca1_lag1','pca2_lag1','pca3_lag1']\n",
    "    X_pc_lag = X_pc_lag.shift(shift_amount)\n",
    "\n",
    "    if parameter_tuning_mode == True:\n",
    "        print(X_pc_lag)\n",
    "        X_pc_lag.shape\n",
    "    return X_pc_lag\n",
    "\n",
    "# Shift the LAG components by 1\n",
    "X_pca_lag1 = create_pca_lag(principal_components_train_test_df, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "bc14d3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shift the LAG components by 2    \n",
    "X_pca_lag2 = create_pca_lag(principal_components_train_test_df, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "24e69a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shift the LAG components by 3\n",
    "X_pca_lag3 = create_pca_lag(principal_components_train_test_df, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "76a3f840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shift the LAG Components by 4\n",
    "X_pca_lag4 = create_pca_lag(principal_components_train_test_df, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "87099fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shift the LAG Components by 5\n",
    "X_pca_lag5 = create_pca_lag(principal_components_train_test_df, 5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "02a3ca85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_lags(X_pc_lag1, X_pc_lag2, X_pc_lag3, X_pc_lag4, X_pc_lag5):\n",
    "    X_pc_lags=pd.concat([X_pc_lag1, \n",
    "                         X_pc_lag2, \n",
    "                         X_pc_lag3, \n",
    "                         X_pc_lag4, \n",
    "                         X_pc_lag5], \n",
    "                         axis=1\n",
    "    )\n",
    "    \n",
    "    if parameter_tuning_mode == True:\n",
    "        X_pc_lags.shape\n",
    "    return X_pc_lags\n",
    "\n",
    "X_pc_lags = concatenate_lags(X_pca_lag1, X_pca_lag2, X_pca_lag3, X_pca_lag4, X_pca_lag5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "5d511169",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_train_test(X_train, X_test):\n",
    "    X_combined = np.concatenate([X_train, X_test], axis = 0)\n",
    "    X_combined = pd.DataFrame(data = X_combined, index=X.index)\n",
    "    return X_combined\n",
    "\n",
    "def concatenate_with_pca_lags(X_raw, X_pc_lags):\n",
    "    # Concatenation of all sources of data. Elimination of NaN due to lag\n",
    "    X_pc = pd.concat([X_raw, X_pc_lags], axis=1)\n",
    "\n",
    "    if parameter_tuning_mode == True:\n",
    "        print(X_pc.shape)\n",
    "    return X_pc\n",
    "\n",
    "X_scaled_df = combine_train_test(X_train_scaled, X_test_scaled)\n",
    "X_pc = concatenate_with_pca_lags(X_scaled_df, X_pc_lags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "a2c0be1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpolation is not applied to numerical variables. We are just going to drop those.\n",
    "if parameter_tuning_mode == True:\n",
    "    print(f\"principal_components_train_test_df.shape: {principal_components_train_test_df.shape}, {principal_components_train_test_df.index.min()}, {principal_components_train_test_df.index.max()} \")\n",
    "    print(f\"X_pc_lags.shape: {X_pc_lags.shape}, {X_pc_lags.index.min()}, {X_pc_lags.index.max()} \")\n",
    "    print(f\"X_pc.shape: {X_pc.shape}, {X_pc.index.min()}, {X_pc.index.max()} \")\n",
    "    print(f\"y.shape: {y.shape}, {y.index.min()}, {y.index.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "2ffeccab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eliminate_nans_in_pca_data(X_pc, y):\n",
    "    X_pc = X_pc[5:-1]\n",
    "    y = y[5:-1]\n",
    "\n",
    "    if parameter_tuning_mode == True:\n",
    "        display(X_pc.shape)\n",
    "        display(y.shape)\n",
    "    return X_pc, y\n",
    "\n",
    "X_pc, y = eliminate_nans_in_pca_data(X_pc, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "1c4aa518",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Redefinition of X and y\n",
    "X = X_pc\n",
    "\n",
    "if parameter_tuning_mode == True:\n",
    "    print(X.shape)\n",
    "    y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "0d578060",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = split_training_test_data(X, y)\n",
    "X_train_scaled, X_test_scaled = standard_scale(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "86c4fe1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting unique columns namess for random over sample model\n",
    "column_name_list = []\n",
    "for i in range(0, len(X.columns)):\n",
    "    column_name_list.append(f\"f_{i}\")\n",
    "X_train_unique_columns = X_train.copy()\n",
    "X_train_unique_columns.columns = column_name_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "7a6a2d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_over_sample(X_train, y_train):\n",
    "    # Use RandomOverSampler to resample the dataset using random_state=1\n",
    "    ros = RandomOverSampler(random_state = 1)\n",
    "    X_train_resampled, y_train_resampled = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "    if parameter_tuning_mode == True:\n",
    "        display(y_train_resampled.value_counts())\n",
    "    return X_train_resampled, y_train_resampled\n",
    "\n",
    "X_train_resampled, y_train_resampled = random_over_sample(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "b8b8cb41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(n_estimators=22)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instance AdaBoost\n",
    "# Initiate the model instance\n",
    "adaboost_model=AdaBoostClassifier(n_estimators=adaboost_estimators)\n",
    "adaboost_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "f1225f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "if parameter_tuning_mode == True:\n",
    "    display(X_train_resampled.shape)\n",
    "    display(y_train_resampled.shape)\n",
    "    display(X_test_scaled.shape)\n",
    "    display(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "7ba135ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model \n",
    "adaboost_model =adaboost_model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "pred_adaboost=adaboost_model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "7db71e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "if demo_mode == False and parameter_tuning_mode == True:\n",
    "    display(np.any(np.isnan(y_test)))\n",
    "    display(np.all(np.isfinite(y_test)))\n",
    "    display(np.any(np.isnan(pred_adaboost)))\n",
    "    display(np.all(np.isfinite(pred_adaboost)))\n",
    "    display(y_test.shape)\n",
    "    display(pred_adaboost.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "a44c0146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         AdaBoost Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.62      0.59      0.61       543\n",
      "         1.0       0.53      0.57      0.55       443\n",
      "\n",
      "    accuracy                           0.58       986\n",
      "   macro avg       0.58      0.58      0.58       986\n",
      "weighted avg       0.58      0.58      0.58       986\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use a classification report to evaluate the model using the predictions and testing data\n",
    "adaboost_report=classification_report(y_test, pred_adaboost)\n",
    "\n",
    "# Print the classification report\n",
    "print(\"         AdaBoost Classification Report\")\n",
    "print(adaboost_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "e5b91bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_features(XY, pca_components):\n",
    "    XY_modified = XY.shift().dropna()\n",
    "    y = XY_modified[\"Signal\"].shift(-1)\n",
    "    X = XY_modified\n",
    "    pca = PCA(n_components = pca_components)\n",
    "    principal_components = pca.fit_transform(X)\n",
    "    \n",
    "    pca_column_list = []\n",
    "    for i in range(1, pca_components+1):\n",
    "        pca_column_list.append(f\"pca{i}\")\n",
    "\n",
    "    principal_components_train_test_df = pd.DataFrame(data = principal_components, columns = pca_column_list, index = XY_modified.index)\n",
    "    X_pca_lag1 = create_pca_lag1(principal_components_train_test_df)\n",
    "    X_pca_lag2 = create_pca_lag2(principal_components_train_test_df)\n",
    "    X_pca_lag3 = create_pca_lag3(principal_components_train_test_df)\n",
    "    X_pca_lag4 = create_pca_lag4(principal_components_train_test_df)\n",
    "    X_pca_lag5 = create_pca_lag5(principal_components_train_test_df)\n",
    "        \n",
    "    X_pc_lags = concatenate_lags(X_pca_lag1, X_pca_lag2, X_pca_lag3, X_pca_lag4, X_pca_lag5)\n",
    "    X_pc = concatenate_pca_with_lags(principal_components_train_test_df, X_pc_lags)\n",
    "    X, y = eliminate_nans_in_pca_data(X_pc, y)\n",
    "    \n",
    "    X_train, y_train, X_test, y_test = split_training_test_data(X, y)\n",
    "    X_train_resampled, y_train_resampled = random_over_sample(X_train, y_train)\n",
    "    X_train_scaled, X_test_scaled = standard_scale(X_train_resampled, X_test)\n",
    "    #principal_components_train_test\n",
    "    if parameter_tuning_mode == True:\n",
    "        display(sum(pca.explained_variance_ratio_))\n",
    "        display(principal_components_train_test_df.shape)\n",
    "    return X_train_scaled, X_test_scaled, y_train_resampled, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "47c2d4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_multiple_tuning_iterations == True: \n",
    "    for num_estimators in range (20,200, 2):\n",
    "        adaboost_model = AdaBoostClassifier(n_estimators=num_estimators)\n",
    "\n",
    "        # Fit the model \n",
    "        adaboost_model = adaboost_model.fit(X_train_resampled, y_train_resampled)\n",
    "        pred_adaboost = adaboost_model.predict(X_test)\n",
    "        # Use a classification report to evaluate the model using the predictions and testing data\n",
    "        adaboost_report=classification_report(y_test, pred_adaboost)\n",
    "\n",
    "        #if num_estimators % 10 == 0 and num_components == 88:\n",
    "        #    print(f\"components {num_components} esimators {num_estimators}\")\n",
    "        #    print(f\"f1 score 0 {f1_score(y_test, pred_adaboost, pos_label=0)} f1 score 1 {f1_score(y_test, pred_adaboost, pos_label=1)}\")\n",
    "        #    print(f\"accuracy {accuracy_score(y_test, pred_adaboost)}\")\n",
    "        #    print(adaboost_report)\n",
    "        f1_score_1 = f1_score(y_test, pred_adaboost, pos_label=1)\n",
    "        f1_score_0 = f1_score(y_test, pred_adaboost, pos_label=0)\n",
    "        recall_score_1 = recall_score(y_test, pred_adaboost, pos_label=1)\n",
    "        recall_score_0 = recall_score(y_test, pred_adaboost, pos_label=0)\n",
    "        accuracy_score_model = accuracy_score(y_test, pred_adaboost)\n",
    "        if  accuracy_score_model >= .55 and f1_score_1 >= .50 and f1_score_0 >= .50 and recall_score_1 >= .50 and recall_score_0 >= .50:\n",
    "            print(f\"estimators {num_estimators}\")\n",
    "            # print(f\"variance explained {sum(pca.explained_variance_ratio_)}\")\n",
    "            # Print the classification report\n",
    "            print(\"         AdaBoost Classification Report\")\n",
    "            print(adaboost_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "eb27a49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of estimators? \n",
    "if run_multiple_tuning_iterations == True:\n",
    "    for n in range (50,200, 10):\n",
    "        # Instance AdaBoost\n",
    "        # Initiate the model instance\n",
    "        adaboost_model=AdaBoostClassifier(n_estimators=n)\n",
    "\n",
    "        # Fit the model \n",
    "        adaboost_model =adaboost_model.fit(X_train_scaled, y_train)\n",
    "        pred_adaboost=adaboost_model.predict(X_test_scaled)\n",
    "        print (n)\n",
    "        # Use a classification report to evaluate the model using the predictions and testing data\n",
    "        adaboost_report=classification_report(y_test, pred_adaboost)\n",
    "\n",
    "        # Print the classification report\n",
    "        print(\"         AdaBoost Classification Report\")\n",
    "        print(adaboost_report)\n",
    "#120 highest 1-recall\n",
    "#150 good overall accuracy, but lower 1-recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96876476",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b74db7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dev)",
   "language": "python",
   "name": "dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
